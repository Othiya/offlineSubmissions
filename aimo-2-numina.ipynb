{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86023,"databundleVersionId":9869096,"sourceType":"competition"},{"sourceId":8149693,"sourceType":"datasetVersion","datasetId":4819800},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":8300737,"sourceType":"datasetVersion","datasetId":4746046},{"sourceId":9650255,"sourceType":"datasetVersion","datasetId":5894287},{"sourceId":201777768,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The winner notebook from the previous competition:\n\nhttps://www.kaggle.com/code/lewtun/numina-1st-place-solution/notebook\n\nChanges:\n\n* removed the code working with the previous competition specific env\n* downloaded the *Numina* model in a separate notebook (aimo-2-numina-model) to run the code offline\n* removed validation set for now\n* decreased the value of **num_samples** from 48 to **19** to make the code run faster\n* the temperature was increased from 0.8 to **0.9**\n","metadata":{}},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport polars as pl\n\nimport kaggle_evaluation.aimo_2_inference_server","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:34:57.485816Z","iopub.execute_input":"2024-10-25T08:34:57.486567Z","iopub.status.idle":"2024-10-25T08:34:58.374966Z","shell.execute_reply.started":"2024-10-25T08:34:57.486512Z","shell.execute_reply":"2024-10-25T08:34:58.374091Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# If using pip\n# !pip install vllm==0.4.2\n# !pip install grpcio==1.62.2\n# !pip install antlr4-python3-runtime==4.11.0\n# !pip install networkx shapely sage matplotlib gmpy2 scipy numpy sympy mpmath\n\n# If on Kaggle\n!pip uninstall -y torch\n!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n!pip install -U --upgrade /kaggle/input/antlr4-python3-runtime-package-4-11/antlr4_python3_runtime-4.11.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:35:01.079011Z","iopub.execute_input":"2024-10-25T08:35:01.080180Z","iopub.status.idle":"2024-10-25T08:37:45.965552Z","shell.execute_reply.started":"2024-10-25T08:35:01.080141Z","shell.execute_reply":"2024-10-25T08:37:45.964554Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.4.0\nUninstalling torch-2.4.0:\n  Successfully uninstalled torch-2.4.0\nLooking in links: /kaggle/input/vllm-whl\nProcessing /kaggle/input/vllm-whl/vllm-0.4.0.post1-cp310-cp310-manylinux1_x86_64.whl\nProcessing /kaggle/input/vllm-whl/cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from vllm) (1.11.1.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from vllm) (5.9.3)\nRequirement already satisfied: ray>=2.9 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.24.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from vllm) (0.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from vllm) (1.26.4)\nProcessing /kaggle/input/vllm-whl/torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (from vllm)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vllm) (2.32.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from vllm) (9.0.0)\nRequirement already satisfied: transformers>=4.39.1 in /opt/conda/lib/python3.10/site-packages (from vllm) (4.45.1)\nProcessing /kaggle/input/vllm-whl/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from vllm) (0.111.0)\nRequirement already satisfied: uvicorn[standard] in /opt/conda/lib/python3.10/site-packages (from vllm) (0.30.1)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.9.2)\nRequirement already satisfied: prometheus-client>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (0.20.0)\nProcessing /kaggle/input/vllm-whl/pynvml-11.5.0-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/vllm-whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from vllm)\nProcessing /kaggle/input/vllm-whl/outlines-0.0.34-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/vllm-whl/tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/input/vllm-whl/interegular-0.3.3-py37-none-any.whl (from outlines==0.0.34->vllm)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (3.1.4)\nProcessing /kaggle/input/vllm-whl/lark-1.1.9-py3-none-any.whl (from outlines==0.0.34->vllm)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.6.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (3.0.0)\nProcessing /kaggle/input/vllm-whl/diskcache-5.6.3-py3-none-any.whl (from outlines==0.0.34->vllm)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.14.1)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (0.60.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.4.2)\nRequirement already satisfied: referencing in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (0.35.1)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (4.22.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken==0.6.0->vllm) (2024.5.15)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (3.15.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (3.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (2024.6.1)\nProcessing /kaggle/input/vllm-whl/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\nProcessing /kaggle/input/vllm-whl/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->vllm)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->vllm) (2.23.4)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (8.1.7)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.0.8)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (21.3)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (3.20.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (6.0.2)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (2024.8.30)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (4.66.4)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.0.4)\nRequirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.27.0)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (5.10.0)\nRequirement already satisfied: orjson>=3.2.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (3.10.4)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (2.1.1)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (12.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->vllm) (2.6.1)\nRequirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.2->fastapi->vllm) (0.12.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi->vllm) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi->vllm) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi->vllm) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->outlines==0.0.34->vllm) (2.1.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray>=2.9->vllm) (3.1.2)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (2023.12.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (0.18.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->outlines==0.0.34->vllm) (0.43.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2->vllm) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->fastapi->vllm) (1.2.0)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (13.7.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (0.1.2)\nInstalling collected packages: triton, pynvml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, interegular, diskcache, cmake, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers, outlines, vllm\n  Attempting uninstall: pynvml\n    Found existing installation: pynvml 11.4.1\n    Uninstalling pynvml-11.4.1:\n      Successfully uninstalled pynvml-11.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\ndask-cuda 24.8.2 requires pynvml<11.5,>=11.0.0, but you have pynvml 11.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.29.0.1 diskcache-5.6.3 interegular-0.3.3 lark-1.1.9 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 outlines-0.0.34 pynvml-11.5.0 tiktoken-0.6.0 torch-2.1.2 triton-2.1.0 vllm-0.4.0.post1 xformers-0.0.23.post1\nProcessing /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\ngrpcio is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (8.1.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (3.15.1)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (4.22.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.0.8)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (21.3)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (3.20.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (6.0.2)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.4.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (2.32.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (0.18.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray==2.11.0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (2024.8.30)\nInstalling collected packages: ray\n  Attempting uninstall: ray\n    Found existing installation: ray 2.24.0\n    Uninstalling ray-2.24.0:\n      Successfully uninstalled ray-2.24.0\nSuccessfully installed ray-2.11.0\nProcessing /kaggle/input/antlr4-python3-runtime-package-4-11/antlr4_python3_runtime-4.11.0-py3-none-any.whl\nInstalling collected packages: antlr4-python3-runtime\nSuccessfully installed antlr4-python3-runtime-4.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport signal\nimport subprocess\nimport tempfile\nfrom collections import Counter\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\n\nimport pandas as pd\nimport polars as pl\nfrom datasets import load_dataset, Dataset, concatenate_datasets\nimport torch\nfrom transformers import set_seed\nfrom tqdm import tqdm\nfrom vllm import LLM, SamplingParams","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:37:55.510756Z","iopub.execute_input":"2024-10-25T08:37:55.511118Z","iopub.status.idle":"2024-10-25T08:37:59.517372Z","shell.execute_reply.started":"2024-10-25T08:37:55.511083Z","shell.execute_reply":"2024-10-25T08:37:59.516602Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-10-25 08:37:59,135\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Configuration\n\nWe found it useful to define a single Config class that gathers all the setting used for a single submission:","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    model_id: str\n\n    # Decoding Parameters\n    num_samples: int        # Number of candidates to generate (width)\n    num_generations: int    # Number of steps to generate per candidate (depth)\n    restart_on_fail: bool   # Regenerate a step if it fails to generate Python codeblocks\n\n    # Sampling Parameters\n    temperature: float\n    max_new_tokens: int\n\n    # Runtime Parameters\n    # validation_set: str  # One of AI-MO/aimo-validation-amc, AI-MO/aimo-validation-aime, AI-MO/aimo-validation-math-level-4, AI-MO/aimo-validation-math-level-5\n    is_submission: bool  # bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n    dtype : str","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:41:11.462242Z","iopub.execute_input":"2024-10-25T08:41:11.462662Z","iopub.status.idle":"2024-10-25T08:41:11.471188Z","shell.execute_reply.started":"2024-10-25T08:41:11.462622Z","shell.execute_reply":"2024-10-25T08:41:11.470333Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### vLLM and model generation utilities","metadata":{}},{"cell_type":"code","source":"def build_vllm(config):\n    num_gpus = torch.cuda.device_count()\n    if \"awq\" in config.model_id.lower():\n        quantization = \"AWQ\"\n    elif \"gptq\" in config.model_id.lower():\n        quantization = \"gptq\"\n    else:\n        quantization = None\n    vllm = LLM(\n        model=config.model_id,\n        tensor_parallel_size=num_gpus,\n        quantization=quantization,\n        swap_space=0,\n    )\n    return vllm\n\n\ndef apply_template(sample, tokenizer, prompt):\n    messages = [{\"role\": \"user\", \"content\": prompt.format(sample[\"prompt\"], \"{}\")}]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    sample[\"text\"] = text\n    return sample\n\n\ndef generate_batched(samples, vllm, sampling_params):\n    outputs = vllm.generate(samples[\"gen_texts\"], sampling_params, use_tqdm=True)\n    samples[\"gen_texts\"] = [o.prompt + o.outputs[0].text for o in outputs]\n    return samples","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:38:08.064085Z","iopub.execute_input":"2024-10-25T08:38:08.064974Z","iopub.status.idle":"2024-10-25T08:38:08.072848Z","shell.execute_reply.started":"2024-10-25T08:38:08.064935Z","shell.execute_reply":"2024-10-25T08:38:08.071880Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Python REPL and code execution utilities","metadata":{}},{"cell_type":"code","source":"class PythonREPL:\n    def __init__(self, timeout=5):\n        self.timeout = timeout\n\n    @contextmanager\n    def time_limit(self, seconds):\n        def signal_handler(*_):\n            raise TimeoutError(f\"Timed out after {seconds} seconds.\")\n\n        signal.signal(signal.SIGALRM, signal_handler)\n        signal.alarm(seconds)\n        try:\n            yield\n        finally:\n            signal.alarm(0)\n\n    def __call__(self, query):\n        query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n        query = query.strip().split(\"\\n\")\n        if \"print(\" not in query[-1]:\n            if \"#\" in query[-1]:\n                query[-1] = query[-1].split(\"#\")[0]\n            query[-1] = \"print(\" + query[-1] + \")\"\n        query = \"\\n\".join(query)\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(query)\n            with self.time_limit(self.timeout):\n                result = subprocess.run(\n                    [\"python3\", temp_file_path],\n                    capture_output=True,\n                    check=False,\n                    text=True,\n                    timeout=self.timeout,\n                )\n                if result.returncode == 0:\n                    output = result.stdout\n                    return True, output.strip()\n                error_msg = result.stderr.strip()\n                msgs = error_msg.split(\"\\n\")\n                new_msgs = []\n                want_next = False\n                for m in msgs:\n                    if \"Traceback\" in m:\n                        new_msgs.append(m)\n                    elif m == msgs[-1]:\n                        new_msgs.append(m)\n                    elif temp_file_path in m:\n                        st = m.index('\"/') + 1 if '\"/' in m else 0\n                        ed = m.index(temp_file_path) + 1 if temp_file_path in m else None\n                        clr = m[st:ed] if not ed else m[st:]\n                        m = m.replace(clr, \"\")\n                        new_msgs.append(m)\n                        want_next = True\n                    elif want_next:\n                        new_msgs.append(m)\n                        want_next = False\n                error_msg = \"\\n\".join(new_msgs)\n                return False, error_msg.strip()\n            \n\ndef execute_completion(executor, completion, return_status, last_code_block):\n    executions = re.findall(r\"```python(.*?)```\", completion, re.DOTALL)\n    if len(executions) == 0:\n        return completion, False if return_status else completion\n    if last_code_block:\n        executions = [executions[-1]]\n    outputs = []\n    successes = []\n    for code in executions:\n        success = False\n        for lib in (\"subprocess\", \"venv\"):\n            if lib in code:\n                output = f\"{lib} is not allowed\"\n                outputs.append(output)\n                successes.append(success)\n                continue\n        try:\n            success, output = executor(code)\n        except TimeoutError as e:\n            print(\"Code timed out\")\n            output = e\n        if not success and not return_status:\n            output = \"\"\n        outputs.append(output)\n        successes.append(success)\n    output = str(outputs[-1]).strip()\n    success = successes[-1]\n    if return_status:\n        return output, success\n    return output\n\n\ndef postprocess_completion(text, return_status, last_code_block):\n    executor = PythonREPL()\n    result = execute_completion(executor, text, return_status=return_status, last_code_block=last_code_block)\n    del executor\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:38:14.813955Z","iopub.execute_input":"2024-10-25T08:38:14.814657Z","iopub.status.idle":"2024-10-25T08:38:14.833969Z","shell.execute_reply.started":"2024-10-25T08:38:14.814616Z","shell.execute_reply":"2024-10-25T08:38:14.833057Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Post-processing and solution extraction utilities","metadata":{}},{"cell_type":"code","source":"def extract_boxed_answer(text):\n    def last_boxed_only_string(text):\n        idx = text.rfind(\"\\\\boxed\")\n        if idx < 0:\n            idx = text.rfind(\"\\\\fbox\")\n            if idx < 0:\n                return None\n        i = idx\n        right_brace_idx = None\n        num_left_braces_open = 0\n        while i < len(text):\n            if text[i] == \"{\":\n                num_left_braces_open += 1\n            if text[i] == \"}\":\n                num_left_braces_open -= 1\n                if num_left_braces_open == 0:\n                    right_brace_idx = i\n                    break\n            i += 1\n        if right_brace_idx is None:\n            return None\n        return text[idx : right_brace_idx + 1]\n\n    def remove_boxed(boxed):\n        left = \"\\\\boxed{\"\n        try:\n            assert boxed[: len(left)] == left\n            assert boxed[-1] == \"}\"\n            length = len(left)\n            return boxed[length:-1]\n        except Exception:\n            return None\n\n    boxed = last_boxed_only_string(text)\n    if boxed is None:\n        return None\n    answer = remove_boxed(boxed)\n    return answer\n\n\ndef normalize_answer(answer):\n    match = re.search(r\"(.*?)Problem:\", answer, flags=re.S)\n    if match:\n        answer = match.group(1)\n    subs = [(\"an \", \"\"), (\"a \", \"\"), (\".$\", \"$\"), (\"\\\\$\", \"\"), (r\"\\ \", \"\"), (\" \", \"\"), (\"mbox\", \"text\"), (\",\\\\text{and}\", \",\"), (\"\\\\text{and}\", \",\"), (\"\\\\text{m}\", \"\\\\text{}\"), (\"\\\\le\", \"<\")]\n    remove = [\"square\", \"ways\", \"integers\", \"dollars\", \"mph\", \"inches\", \"ft\", \"hours\", \"km\", \"units\", \"\\\\ldots\", \"sue\", \"points\", \"feet\", \"minutes\", \"digits\", \"cents\", \"degrees\", \"cm\", \"gm\", \"pounds\", \"meters\", \"meals\", \"edges\", \"students\", \"childrentickets\", \"multiples\", \"\\\\text{s}\", \"\\\\text{.}\", \"\\\\text{\\ns}\", \"\\\\text{}^2\", \"\\\\text{}^3\", \"\\\\text{\\n}\", \"\\\\text{}\", r\"\\mathrm{th}\", r\"^\\circ\", r\"^{\\circ}\", r\"\\;\", r\",\\!\", \"{,}\", '\"', \"\\\\dots\", \"\\n\", \"\\r\", \"\\f\", \"\\%\"]\n    sub_patterns = [r\"(\\\\text\\{)(.*?)(\\})\", r\"(\\\\textbf\\{)(.*?)(\\})\", r\"(\\\\overline\\{)(.*?)(\\})\", r\"(\\\\boxed\\{)(.*)(\\})\"]\n    split_patterns = [r\"finalansweris(.*)\", r\"answer?is:?(.*)\", r\"oxed\\{(.*?)\\}\", r\"\\$(.*?)\\$\"]\n    for before, after in subs:\n        answer = answer.replace(before, after)\n    for expr in remove:\n        answer = answer.replace(expr, \"\")\n    for pattern in sub_patterns:\n        answer = re.sub(pattern, \"\\\\2\", answer)\n    for pattern in split_patterns:\n        if len(re.findall(pattern, answer)) > 0:\n            answer = re.findall(pattern, answer)[-1]\n    answer = answer.strip()\n    if \"rac\" in answer and \"\\\\frac\" not in answer:\n        answer = answer.replace(\"rac\", \"\\\\frac\")\n    answer = re.sub(r\"(frac)([^{])(.)\", \"frac{\\\\2}{\\\\3}\", answer)\n    answer = re.sub(r\"(sqrt)([^{])\", \"sqrt{\\\\2}\", answer)\n    answer = answer.replace(\"$\", \"\")\n    if answer.replace(\",\", \"\").isdigit():\n        answer = answer.replace(\",\", \"\")\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:38:18.831189Z","iopub.execute_input":"2024-10-25T08:38:18.831573Z","iopub.status.idle":"2024-10-25T08:38:18.848148Z","shell.execute_reply.started":"2024-10-25T08:38:18.831537Z","shell.execute_reply":"2024-10-25T08:38:18.847210Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### SC-TIR control flow","metadata":{}},{"cell_type":"code","source":"def process_code(sample, restart_on_fail, last_step, check_last_n_chars=100):\n    gen_text = sample[\"gen_texts\"]\n    num_python_blocks = len(re.findall(r\"```python(.*?)```\", gen_text, re.DOTALL))\n    region_to_check = gen_text[-check_last_n_chars:]\n    if num_python_blocks == 0:\n        if restart_on_fail:\n            print(\"no code has ever been generated, RESTARTING\")\n            sample[\"gen_texts\"] = sample[\"text\"]\n        else:\n            print(\"no code has ever been generated, STOP\")\n            sample[\"should_prune\"] = True\n            sample[\"has_code\"] = False\n        return sample\n    if not gen_text.endswith(\"```output\\n\") and (\"answer is\" in region_to_check or \"\\\\boxed\" in region_to_check):\n        num_output_blocks = len(re.findall(r\"```output(.*?)```\", gen_text, re.DOTALL))\n        if num_output_blocks == 0:\n            print(\"The model hallucinated the code answer\")\n            sample[\"should_prune\"] = True\n            return sample\n        if \"boxed\" in region_to_check:\n            try:\n                answer = normalize_answer(extract_boxed_answer(region_to_check))\n            except Exception:\n                answer = \"-1\"\n        else:\n            answer = normalize_answer(region_to_check)\n        sample[\"model_answers\"] = answer\n        return sample\n    if last_step:\n        return sample\n    if not gen_text.endswith(\"```output\\n\"):\n        print(\"warning: output block not found: \", gen_text[-40:])\n        if restart_on_fail:\n            sample[\"gen_texts\"] = sample[\"text\"]\n        else:\n            sample[\"should_prune\"] = True\n        return sample\n    code_result, _ = postprocess_completion(gen_text, return_status=True, last_code_block=True)\n    truncation_limit = 200\n    if len(code_result) > truncation_limit:\n        code_result = code_result[:truncation_limit] + \" ... (output truncated)\"\n    sample[\"gen_texts\"] = gen_text + f\"{code_result}\\n```\"\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:38:22.920993Z","iopub.execute_input":"2024-10-25T08:38:22.921615Z","iopub.status.idle":"2024-10-25T08:38:22.932418Z","shell.execute_reply.started":"2024-10-25T08:38:22.921574Z","shell.execute_reply":"2024-10-25T08:38:22.931507Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Sample filtering and majority voting","metadata":{}},{"cell_type":"code","source":"def filter_answers(answers):\n    def validate_answer_is_numeric(x, tolerance=0.2):\n        try:\n            x = round(float(x))\n            f = float(x)\n            if abs(x - f) > tolerance:\n                x = -1\n        except Exception:\n            x = -1\n        return x\n\n    formatted = [validate_answer_is_numeric(a) for a in answers]\n    filtered = [a for a in formatted if a >= 0]\n    return filtered\n\n\ndef get_majority_vote(answers):\n    if not len(answers):\n        return 0\n    c = Counter(answers)\n    value, _ = c.most_common()[0]\n    return value","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:38:26.714633Z","iopub.execute_input":"2024-10-25T08:38:26.714996Z","iopub.status.idle":"2024-10-25T08:38:26.722051Z","shell.execute_reply.started":"2024-10-25T08:38:26.714962Z","shell.execute_reply":"2024-10-25T08:38:26.721100Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Specify config","metadata":{}},{"cell_type":"code","source":"config = Config(\n    # model_id=\"AI-MO/NuminaMath-7B-TIR-GPTQ\",\n    model_id=\"Qwen/Qwen2.5-Math-7B-Instruct\",\n    num_samples=19,  # 48,\n    num_generations=4,\n    restart_on_fail=True,\n    temperature=0.9,\n    max_new_tokens=2048,\n    # validation_set=\"AI-MO/aimo-validation-amc\",\n    is_submission=False,\n    dtype=\"half\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:41:22.381190Z","iopub.execute_input":"2024-10-25T08:41:22.381889Z","iopub.status.idle":"2024-10-25T08:41:22.386716Z","shell.execute_reply.started":"2024-10-25T08:41:22.381849Z","shell.execute_reply":"2024-10-25T08:41:22.385753Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(f\"=== Running submission with config ===\\n\\n{config}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:41:25.085545Z","iopub.execute_input":"2024-10-25T08:41:25.085919Z","iopub.status.idle":"2024-10-25T08:41:25.090988Z","shell.execute_reply.started":"2024-10-25T08:41:25.085883Z","shell.execute_reply":"2024-10-25T08:41:25.089934Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"=== Running submission with config ===\n\nConfig(model_id='Qwen/Qwen2.5-Math-7B-Instruct', num_samples=19, num_generations=4, restart_on_fail=True, temperature=0.9, max_new_tokens=2048, is_submission=False, dtype='half')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Run computations","metadata":{}},{"cell_type":"code","source":"set_seed(42)\nnum_procs = os.cpu_count()\nvllm = build_vllm(config)\nsampling_params = SamplingParams(\n    temperature=config.temperature,\n    max_tokens=config.max_new_tokens,\n    stop=[\"```output\\n\"],\n    include_stop_str_in_output=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:38:38.285205Z","iopub.execute_input":"2024-10-25T08:38:38.285600Z","iopub.status.idle":"2024-10-25T08:39:05.183958Z","shell.execute_reply.started":"2024-10-25T08:38:38.285563Z","shell.execute_reply":"2024-10-25T08:39:05.182565Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e9be22e95d42c3804f0eb0f175446a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = _posixsubprocess.fork_exec(\n/opt/conda/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = _posixsubprocess.fork_exec(\n2024-10-25 08:38:51,890\tINFO worker.py:1749 -- Started a local Ray instance.\n","output_type":"stream"},{"name":"stdout","text":"INFO 10-25 08:38:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='Qwen/Qwen2.5-Math-7B-Instruct', tokenizer='Qwen/Qwen2.5-Math-7B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23cc2d9835294280bbef62dac68cb0bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f248c70d98464d2f8715286cc54cdb9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2da5a0b14ce405eae916f8ea0bcd259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da7140869df46bb9241a619c6678824"}},"metadata":{}},{"name":"stdout","text":"INFO 10-25 08:39:03 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\nINFO 10-25 08:39:03 selector.py:25] Using XFormers backend.\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m INFO 10-25 08:39:04 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m INFO 10-25 08:39:04 selector.py:25] Using XFormers backend.\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44] Error executing method init_device. This might cause deadlock in distributed execution.\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44] Traceback (most recent call last):\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/ray_utils.py\", line 37, in execute_method\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44]     return executor(*args, **kwargs)\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 93, in init_device\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44]     _check_if_gpu_supports_dtype(self.model_config.dtype)\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 312, in _check_if_gpu_supports_dtype\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44]     raise ValueError(\n\u001b[36m(RayWorkerVllm pid=409)\u001b[0m ERROR 10-25 08:39:04 ray_utils.py:44] ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m num_procs \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mcpu_count()\n\u001b[0;32m----> 3\u001b[0m vllm \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m      5\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m      6\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_new_tokens,\n\u001b[1;32m      7\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```output\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     include_stop_str_in_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m )\n","Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mbuild_vllm\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     quantization \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m vllm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mswap_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vllm\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/entrypoints/llm.py:112\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     94\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     95\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     96\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    111\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py:196\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    193\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py:110\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenizer \u001b[38;5;241m=\u001b[39m Detokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py:62\u001b[0m, in \u001b[0;36mRayGPUExecutor.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config)\u001b[0m\n\u001b[1;32m     59\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAY_USAGE_STATS_ENABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Create the parallel GPU workers.\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_workers_ray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplacement_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cache()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py:191\u001b[0m, in \u001b[0;36mRayGPUExecutor._init_workers_ray\u001b[0;34m(self, placement_group, **ray_remote_kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m driver_local_rank \u001b[38;5;241m=\u001b[39m node_workers[driver_node_id]\u001b[38;5;241m.\u001b[39mindex(driver_rank)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m Worker(\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     is_driver_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    194\u001b[0m     max_concurrent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    195\u001b[0m     max_parallel_loading_workers,\n\u001b[1;32m    196\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py:324\u001b[0m, in \u001b[0;36mRayGPUExecutor._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, use_ray_compiled_dag, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py:93\u001b[0m, in \u001b[0;36mWorker.init_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 93\u001b[0m \u001b[43m_check_if_gpu_supports_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_gpu_memory \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmem_get_info()[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py:312\u001b[0m, in \u001b[0;36m_check_if_gpu_supports_dtype\u001b[0;34m(torch_dtype)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_capability[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m    311\u001b[0m     gpu_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name()\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBfloat16 is only supported on GPUs with compute capability \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof at least 8.0. Your \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPU has compute capability \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompute_capability[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompute_capability[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can use float16 instead by explicitly setting the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dtype` flag in CLI, for example: --dtype=half.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half."],"ename":"ValueError","evalue":"Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.","output_type":"error"},{"name":"stderr","text":"2024-10-25 08:39:10,357\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RayWorkerVllm.execute_method()\u001b[39m (pid=409, ip=172.19.2.2, actor_id=a9cc01cbbff0a1a1b970bf0e01000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f4799044910>)\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/ray_utils.py\", line 45, in execute_method\n    raise e\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/ray_utils.py\", line 37, in execute_method\n    return executor(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 93, in init_device\n    _check_if_gpu_supports_dtype(self.model_config.dtype)\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 312, in _check_if_gpu_supports_dtype\n    raise ValueError(\nValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n","output_type":"stream"}]},{"cell_type":"code","source":"def solve_problem(question: str):\n    assert type(question) is str\n    problem = apply_template(\n        {\"prompt\": question},\n        tokenizer=vllm.get_tokenizer(),\n        prompt=\"{}\"\n    )\n    samples = Dataset.from_list([\n        {\n            \"text\": problem[\"text\"],\n            \"gen_texts\": problem[\"text\"],\n            \"should_prune\": False,\n            \"model_answers\": \"-1\",\n            \"has_code\": True,\n        }\n        for _ in range(config.num_samples)\n    ])\n    completed = []\n    for step in range(config.num_generations):\n        samples = samples.map(\n            generate_batched,\n            batch_size=128,\n            batched=True,\n            fn_kwargs={\n                \"vllm\": vllm,\n                \"sampling_params\": sampling_params\n            },\n            load_from_cache_file=False,\n        )\n        samples = samples.map(\n            process_code,\n            num_proc=num_procs,\n            load_from_cache_file=False,\n            fn_kwargs={\n                \"restart_on_fail\": config.restart_on_fail,\n                \"last_step\": step == (config.num_generations - 1)\n            },\n        )\n        done = samples.filter(\n            lambda x: x[\"should_prune\"] is True,\n            load_from_cache_file=False\n        )\n        if len(done):\n            completed.append(done)\n        samples = samples.filter(\n            lambda x: x[\"should_prune\"] is False,\n            load_from_cache_file=False\n        )\n    completed.append(samples)\n    samples = concatenate_datasets(completed)\n    candidates = samples[\"model_answers\"]\n    print(f\"=== CANDIDATE ANSWERS ({len(candidates)}) ===\\n{candidates}\\n\")\n    filtered = filter_answers(candidates)\n    print(f\"=== FILTERED ANSWERS ({len(filtered)}) ===\\n{filtered}\\n\")\n    majority = get_majority_vote(filtered)\n    print(f\"=== MAJORITY ANSWER (mod 1000) ===\\n{majority}\\n\")\n    return majority","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:27:47.702859Z","iopub.execute_input":"2024-10-25T08:27:47.703815Z","iopub.status.idle":"2024-10-25T08:27:47.716300Z","shell.execute_reply.started":"2024-10-25T08:27:47.703767Z","shell.execute_reply":"2024-10-25T08:27:47.715194Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the `predict` function. When we evaluate your submission on the hidden test set the client defined in `aimo_2_gateway` will run in a different container with direct access to the hidden test set and hand off each question one at a time, in random order.\n\nYour code will always have access to the published copies of the files.","metadata":{}},{"cell_type":"code","source":"# Replace this function with your inference code.\n# The function should return a single integer between 0 and 999, inclusive.\n# Each prediction (except the very first) must be returned within 30 minutes\n# of the question being provided.\n\ndef predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n    print('Types:', type(id_), type(question))\n    id_str = id_.item(0)\n    question_str = question.item(0)\n\n    assert type(id_str) is str\n    assert type(question_str) is str\n\n    print('====================================================')\n    print('QUESTION:', question_str)\n\n    prediction = solve_problem(question_str)\n    print('PREDICTION:', prediction)\n    print('====================================================')\n\n    return pl.DataFrame({'id': id_str, 'answer': prediction})","metadata":{"execution":{"iopub.status.busy":"2024-10-18T09:54:57.57285Z","iopub.execute_input":"2024-10-18T09:54:57.573208Z","iopub.status.idle":"2024-10-18T09:54:57.587424Z","shell.execute_reply.started":"2024-10-18T09:54:57.573159Z","shell.execute_reply":"2024-10-18T09:54:57.586524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When your notebook is run on the hidden test set, `inference_server.serve()` must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first predict call, which does not have the usual 10 minute response deadline.","metadata":{}},{"cell_type":"code","source":"inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-18T09:54:57.58909Z","iopub.execute_input":"2024-10-18T09:54:57.590445Z","iopub.status.idle":"2024-10-18T09:56:24.104376Z","shell.execute_reply.started":"2024-10-18T09:54:57.590396Z","shell.execute_reply":"2024-10-18T09:56:24.103224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom sklearn.metrics import accuracy_score\n\n\nif not config.is_submission:\n    reference = pd.read_csv(\n        '/kaggle/input/translated-test-df/translated_test_df.csv'\n    )\n    true_answers = []\n    pred_answers = []\n\n    for id_, row in reference.iterrows():\n#         true_answers.append(row['answer'])\n        id_no = row['ID']\n        print('Solving for id : ', id_no)\n        pred_answers.append(solve_problem(row['Problem']))\n    \n    #print('accuracy:', accuracy_score(true_answers, pred_answers))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:29:15.496034Z","iopub.execute_input":"2024-10-25T08:29:15.496492Z","iopub.status.idle":"2024-10-25T08:32:31.184280Z","shell.execute_reply.started":"2024-10-25T08:29:15.496453Z","shell.execute_reply":"2024-10-25T08:32:31.182987Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Parameter 'fn_kwargs'={'vllm': <vllm.entrypoints.llm.LLM object at 0x7cfb98b48b20>, 'sampling_params': SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['```output\\n'], stop_token_ids=[], include_stop_str_in_output=True, ignore_eos=False, max_tokens=2048, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True)} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"name":"stdout","text":"Solving for id :  0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c2862069c9429093f0a278dd1104c1"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\nProcessed prompts:   5%|         | 1/19 [00:15<04:37, 15.41s/it]\u001b[A\nProcessed prompts:  11%|         | 2/19 [00:16<01:54,  6.75s/it]\u001b[A\nProcessed prompts:  21%|        | 4/19 [00:16<00:41,  2.74s/it]\u001b[A\nProcessed prompts:  26%|       | 5/19 [00:17<00:29,  2.11s/it]\u001b[A\nProcessed prompts:  32%|      | 6/19 [00:17<00:20,  1.58s/it]\u001b[A\nProcessed prompts:  37%|      | 7/19 [00:18<00:15,  1.29s/it]\u001b[A\nProcessed prompts:  42%|     | 8/19 [00:18<00:11,  1.01s/it]\u001b[A\nProcessed prompts:  47%|     | 9/19 [00:19<00:07,  1.25it/s]\u001b[A\nProcessed prompts:  53%|    | 10/19 [00:19<00:07,  1.28it/s]\u001b[A\nProcessed prompts:  58%|    | 11/19 [00:20<00:05,  1.49it/s]\u001b[A\nProcessed prompts:  63%|   | 12/19 [00:21<00:04,  1.46it/s]\u001b[A\nProcessed prompts:  68%|   | 13/19 [00:21<00:03,  1.77it/s]\u001b[A\nProcessed prompts:  74%|  | 14/19 [00:21<00:02,  2.20it/s]\u001b[A\nProcessed prompts:  84%| | 16/19 [00:21<00:00,  3.70it/s]\u001b[A\nProcessed prompts:  95%|| 18/19 [00:22<00:00,  3.67it/s]\u001b[A\nProcessed prompts: 100%|| 19/19 [00:22<00:00,  1.20s/it]\u001b[A\n/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36a99df4414e46718f1b6caa5dd413ef"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9193116dbc0841fb8fb2908a7d03852c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01447a618f7a4811996911dac9fb7cfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc810fa3bc14be4a068e9b57d39d26c"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\nProcessed prompts:   5%|         | 1/19 [00:03<01:08,  3.82s/it]\u001b[A\nProcessed prompts:  26%|       | 5/19 [00:03<00:08,  1.67it/s]\u001b[A\nProcessed prompts:  37%|      | 7/19 [00:04<00:04,  2.44it/s]\u001b[A\nProcessed prompts:  58%|    | 11/19 [00:04<00:01,  4.08it/s]\u001b[A\nProcessed prompts:  68%|   | 13/19 [00:04<00:01,  4.51it/s]\u001b[A\nProcessed prompts:  79%|  | 15/19 [00:05<00:00,  4.14it/s]\u001b[A\nProcessed prompts:  95%|| 18/19 [00:06<00:00,  2.92it/s]\u001b[A\nProcessed prompts: 100%|| 19/19 [00:07<00:00,  2.43it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62dedd1e80d24b7b800384e9d9a0017e"}},"metadata":{}},{"name":"stdout","text":"warning: output block not found:  eck, you can use this formula directly.\n\nwarning: output block not found: warning: output block not found:   maximum number of new pieces each time.\n\nber of pieces with each additional cut.\n\nwarning: output block not found: \n the maximum number of separate regions.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6876d1bea62949c0a8eecc26f8a24c7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f2d3f37d59c43c883da2bed04b054fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f561fd3ab2541adb48ab1aa57f13597"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\nProcessed prompts:   5%|         | 1/19 [00:01<00:23,  1.29s/it]\u001b[A\nProcessed prompts:  53%|    | 10/19 [00:01<00:01,  7.02it/s]\u001b[A\nProcessed prompts:  68%|   | 13/19 [00:02<00:01,  4.94it/s]\u001b[A\nProcessed prompts:  74%|  | 14/19 [00:03<00:01,  3.09it/s]\u001b[A\nProcessed prompts:  84%| | 16/19 [00:08<00:02,  1.23it/s]\u001b[A\nProcessed prompts:  89%| | 17/19 [00:10<00:02,  1.02s/it]\u001b[A\nProcessed prompts:  95%|| 18/19 [00:10<00:00,  1.08it/s]\u001b[A\nProcessed prompts: 100%|| 19/19 [00:13<00:00,  1.46it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5fa73cdd07146a6af3799cbc81a1e21"}},"metadata":{}},{"name":"stdout","text":"warning: output block not found:  \n\n\\[ P = 2^2 - 2 + 1 = 4 - 2 + 1 = 3 \\]\n\nwarning: output block not found: warning: output block not found:  , you can obtain a maximum of 4 pieces.\n \nally, can create a maximum of 3 pieces.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"608b53aa9f464001b7b6d16fd27516c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f169c7b48541f7b0ecf2e6e0c2b67c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34a85eaa3472430eb6571dca5ef9e321"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\nProcessed prompts:   5%|         | 1/19 [00:01<00:23,  1.29s/it]\u001b[A\nProcessed prompts:  47%|     | 9/19 [00:01<00:01,  5.78it/s]\u001b[A\nProcessed prompts:  68%|   | 13/19 [00:02<00:01,  4.96it/s]\u001b[A\nProcessed prompts:  89%| | 17/19 [00:07<00:01,  1.89it/s]\u001b[A\nProcessed prompts:  95%|| 18/19 [00:08<00:00,  1.70it/s]\u001b[A\nProcessed prompts: 100%|| 19/19 [00:08<00:00,  2.11it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa66ed0e3cfe48d2b42cc222edf4a597"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ebc609e2e0403ab5d558ffbac22b5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a8b7c454d744b2b844e51138aedc50"}},"metadata":{}},{"name":"stdout","text":"=== CANDIDATE ANSWERS (19) ===\n['3', '4', '3', '3', '3', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '3', '4']\n\n=== FILTERED ANSWERS (19) ===\n[3, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4]\n\n=== MAJORITY ANSWER (mod 1000) ===\n4\n\nSolving for id :  1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"381e873db70e4780aba7777224520c8d"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\nProcessed prompts:   5%|         | 1/19 [00:29<08:42, 29.00s/it]\u001b[A\nProcessed prompts:  11%|         | 2/19 [00:29<03:24, 12.04s/it]\u001b[A\nProcessed prompts:  16%|        | 3/19 [00:29<01:47,  6.74s/it]\u001b[A\nProcessed prompts:  21%|        | 4/19 [00:30<01:07,  4.52s/it]\u001b[A\nProcessed prompts:  26%|       | 5/19 [00:33<00:52,  3.77s/it]\u001b[A\nProcessed prompts:  32%|      | 6/19 [00:33<00:35,  2.73s/it]\u001b[A\nProcessed prompts:  42%|     | 8/19 [00:36<00:21,  1.91s/it]\u001b[A\nProcessed prompts:  53%|    | 10/19 [00:36<00:11,  1.23s/it]\u001b[A\nProcessed prompts:  58%|    | 11/19 [00:37<00:09,  1.21s/it]\u001b[A\nProcessed prompts:  68%|   | 13/19 [00:37<00:04,  1.27it/s]\u001b[A\nProcessed prompts:  74%|  | 14/19 [00:38<00:03,  1.26it/s]\u001b[A\nProcessed prompts:  84%| | 16/19 [00:39<00:01,  1.85it/s]\u001b[A\nProcessed prompts:  89%| | 17/19 [00:39<00:00,  2.12it/s]\u001b[A\nProcessed prompts:  95%|| 18/19 [00:39<00:00,  2.54it/s]\u001b[A\nProcessed prompts: 100%|| 19/19 [00:41<00:00,  2.16s/it]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f5be7a19774a70ae90afc82d83e7a6"}},"metadata":{}},{"name":"stderr","text":"*** SIGTERM received at time=1729845070 on cpu 3 ***\n*** SIGTERM received at time=1729845070 on cpu 1 ***\nPC: @     0x7cfcd7bdfd0f  (unknown)  sem_trywait\n    @     0x7cfcd7b85520  (unknown)  (unknown)\n    @ ... and at least 1 more frames\n[2024-10-25 08:31:10,939 E 813 30] logging.cc:365: *** SIGTERM received at time=1729845070 on cpu 1 ***\n[2024-10-25 08:31:10,939 E 813 30] logging.cc:365: PC: @     0x7cfcd7bdfd0f  (unknown)  sem_trywait\n[2024-10-25 08:31:10,940 E 813 30] logging.cc:365:     @     0x7cfcd7b85520  (unknown)  (unknown)\n[2024-10-25 08:31:10,940 E 813 30] logging.cc:365:     @ ... and at least 1 more frames\nPC: @     0x57c4cc7080ab  (unknown)  _PyObject_GetMethod\n    @     0x7cfcd7b85520  (unknown)  (unknown)\n    @     0x57c4ce3f3ac0  (unknown)  (unknown)\n[2024-10-25 08:31:10,954 E 812 30] logging.cc:365: *** SIGTERM received at time=1729845070 on cpu 3 ***\n[2024-10-25 08:31:10,954 E 812 30] logging.cc:365: PC: @     0x57c4cc7080ab  (unknown)  _PyObject_GetMethod\n[2024-10-25 08:31:10,962 E 812 30] logging.cc:365:     @     0x7cfcd7b85520  (unknown)  (unknown)\n[2024-10-25 08:31:10,973 E 812 30] logging.cc:365:     @     0x57c4ce3f3ac0  (unknown)  (unknown)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1549303e81ed40d49ce383b3cb421327"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb0eb145a2984823a945bb553e7f1915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2dad0923456453fbab5c021871a37ae"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\nProcessed prompts:   5%|         | 1/19 [00:05<01:38,  5.46s/it]\u001b[A\nProcessed prompts:  16%|        | 3/19 [00:05<00:23,  1.48s/it]\u001b[A\nProcessed prompts:  37%|      | 7/19 [00:05<00:06,  2.00it/s]\u001b[A\nProcessed prompts:  47%|     | 9/19 [00:06<00:03,  2.54it/s]\u001b[A\nProcessed prompts:  58%|    | 11/19 [00:06<00:02,  3.01it/s]\u001b[A\nProcessed prompts:  68%|   | 13/19 [00:07<00:02,  2.85it/s]\u001b[A\nProcessed prompts:  74%|  | 14/19 [00:07<00:01,  2.62it/s]\u001b[A\nProcessed prompts:  79%|  | 15/19 [00:10<00:03,  1.33it/s]\u001b[A\nProcessed prompts:  84%| | 16/19 [00:12<00:03,  1.07s/it]\u001b[A\nProcessed prompts:  89%| | 17/19 [00:12<00:01,  1.02it/s]\u001b[A\nProcessed prompts:  95%|| 18/19 [00:14<00:01,  1.23s/it]\u001b[A\nProcessed prompts: 100%|| 19/19 [00:15<00:00,  1.21it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b78fa4821947b88f433eb1bbb7891b"}},"metadata":{}},{"name":"stdout","text":"warning: output block not found:  considering the given jumping patterns.\n\nwarning: output block not found:  ered ducks have jumped on these stones.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"769d30ca0f6942da9f4407c8bc35acba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e51332e47f44bd8f059586cef6413a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5fcc6cedb1543ebb6cdc326e6e78d09"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\nProcessed prompts:   5%|         | 1/19 [00:01<00:23,  1.29s/it]\u001b[A\nProcessed prompts:  26%|       | 5/19 [00:02<00:06,  2.06it/s]\u001b[A\nProcessed prompts:  58%|    | 11/19 [00:04<00:02,  3.14it/s]\u001b[A\nProcessed prompts:  79%|  | 15/19 [00:04<00:01,  3.47it/s]\u001b[A\nProcessed prompts:  84%| | 16/19 [00:05<00:00,  3.66it/s]\u001b[A\nProcessed prompts:  89%| | 17/19 [00:06<00:00,  2.61it/s]\u001b[A\nProcessed prompts:  95%|| 18/19 [00:16<00:02,  2.06s/it]\u001b[A\nProcessed prompts: 100%|| 19/19 [00:18<00:00,  1.01it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d097b8f36c16483ebc279edd0f38d7e7"}},"metadata":{}},{"name":"stdout","text":"warning: output block not found:  en landed on by an odd number of ducks.\n\nwarning: output block not found:  hows that there are \\(50\\) such stones.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd667854eb144342bfb112defaf08ac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baec70ae3b144c5d8ce3a51e65045d59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c1f3882cbff4561b20a2314f8fc73b8"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\nProcessed prompts:   5%|         | 1/19 [00:01<00:24,  1.34s/it]\u001b[A\nProcessed prompts:  32%|      | 6/19 [00:02<00:05,  2.55it/s]\u001b[A\nProcessed prompts:  58%|    | 11/19 [00:03<00:02,  3.22it/s]\u001b[A\nProcessed prompts:  84%| | 16/19 [00:04<00:00,  4.24it/s]\u001b[A\nProcessed prompts:  95%|| 18/19 [00:16<00:01,  1.37s/it]\u001b[A\nProcessed prompts: 100%|| 19/19 [00:20<00:00,  1.06s/it]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e886beb499314ca591af6636ff890bc8"}},"metadata":{}},{"name":"stderr","text":"*** SIGTERM received at time=1729845132 on cpu 2 ***\nPC: @     0x7cfcd7bdf4a5  (unknown)  sem_post\n    @     0x7cfcd7b85520  (unknown)  (unknown)\n[2024-10-25 08:32:12,807 E 1013 30] logging.cc:365: *** SIGTERM received at time=1729845132 on cpu 2 ***\n[2024-10-25 08:32:12,807 E 1013 30] logging.cc:365: PC: @     0x7cfcd7bdf4a5  (unknown)  sem_post\n[2024-10-25 08:32:12,808 E 1013 30] logging.cc:365:     @     0x7cfcd7b85520  (unknown)  (unknown)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff92d2234f234d2393e9f75dd6e792c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d876f01d5444a6b4767352ae83edbb"}},"metadata":{}},{"name":"stdout","text":"=== CANDIDATE ANSWERS (19) ===\n['49', '100', '49', '50', '49', '49', '0', '10', '7', '10', '275', '275', '150', '50', '10', '10', '100', '84', '17']\n\n=== FILTERED ANSWERS (19) ===\n[49, 100, 49, 50, 49, 49, 0, 10, 7, 10, 275, 275, 150, 50, 10, 10, 100, 84, 17]\n\n=== MAJORITY ANSWER (mod 1000) ===\n49\n\nSolving for id :  2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499367be75b14da6ac944c02d55bb74c"}},"metadata":{}},{"name":"stderr","text":"\nProcessed prompts:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed exec>:15\u001b[0m\n","Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36msolve_problem\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     18\u001b[0m completed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_generations):\n\u001b[0;32m---> 20\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerate_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvllm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     31\u001b[0m         process_code,\n\u001b[1;32m     32\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39mnum_procs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m         },\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     done \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould_prune\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         load_from_cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3035\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3036\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3037\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3438\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3434\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3435\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3436\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3438\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3442\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3300\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3299\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3300\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3302\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3303\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3304\u001b[0m     }\n","Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36mgenerate_batched\u001b[0;34m(samples, vllm, sampling_params)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_batched\u001b[39m(samples, vllm, sampling_params):\n\u001b[0;32m---> 26\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgen_texts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39mprompt \u001b[38;5;241m+\u001b[39m o\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m samples\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/entrypoints/llm.py:190\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, multi_modal_data)\u001b[0m\n\u001b[1;32m    177\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    178\u001b[0m         i]\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(\n\u001b[1;32m    180\u001b[0m         prompt,\n\u001b[1;32m    181\u001b[0m         sampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m multi_modal_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     )\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/entrypoints/llm.py:218\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    216\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 218\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py:676\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[0;32m--> 676\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    681\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py:260\u001b[0m, in \u001b[0;36mRayGPUExecutor.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    256\u001b[0m                   seq_group_metadata_list: List[SequenceGroupMetadata],\n\u001b[1;32m    257\u001b[0m                   blocks_to_swap_in: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    258\u001b[0m                   blocks_to_swap_out: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    259\u001b[0m                   blocks_to_copy: Dict[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SamplerOutput:\n\u001b[0;32m--> 260\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq_group_metadata_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_copy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ray_compiled_dag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_RAY_COMPILED_DAG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     output \u001b[38;5;241m=\u001b[39m all_outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py:324\u001b[0m, in \u001b[0;36mRayGPUExecutor._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, use_ray_compiled_dag, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py:221\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 221\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py:673\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:360\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    357\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    358\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 360\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:76\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m     73\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Get the logprobs query results.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m prompt_logprobs, sample_logprobs \u001b[38;5;241m=\u001b[39m _get_logprobs(\n\u001b[1;32m     80\u001b[0m     logprobs, sampling_metadata, sample_results)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:502\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    497\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    498\u001b[0m     logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    499\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    500\u001b[0m     sampling_tensors: SamplingTensors,\n\u001b[1;32m    501\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:401\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata)\u001b[0m\n\u001b[1;32m    399\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _greedy_sample(seq_groups, greedy_samples)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[0;32m--> 401\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mBEAM:\n\u001b[1;32m    404\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _beam_search_sample(seq_groups, is_prompts,\n\u001b[1;32m    405\u001b[0m                                          sampling_metadata\u001b[38;5;241m.\u001b[39mseq_data,\n\u001b[1;32m    406\u001b[0m                                          beam_search_logprobs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:235\u001b[0m, in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, is_prompts, random_samples)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_random_sample\u001b[39m(\n\u001b[1;32m    230\u001b[0m     selected_seq_groups: List[Tuple[List[\u001b[38;5;28mint\u001b[39m], SamplingParams]],\n\u001b[1;32m    231\u001b[0m     is_prompts: List[\u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m    232\u001b[0m     random_samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    233\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# Find the maximum best_of value of the prompt phase requests.\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     random_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    237\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}